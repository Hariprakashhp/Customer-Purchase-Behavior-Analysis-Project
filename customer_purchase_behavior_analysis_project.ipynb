{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM3GKvEzPFwzwxP39Y/HseF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hariprakashhp/Customer-Purchase-Behavior-Analysis-Project/blob/main/customer_purchase_behavior_analysis_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fRbTL0orB3yk"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset into a pandas DataFrame\n",
        "pd = pd.read_csv('/content/ecommerce_customer_data_custom_ratios.csv')"
      ],
      "metadata": {
        "id": "YuvwxFmqTA7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the first few rows of the DataFrame to understand its structure\n",
        "pd.head()"
      ],
      "metadata": {
        "id": "2_qFSEt4TsOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get information about the DataFrame, including data types and non-null counts\n",
        "pd.info()"
      ],
      "metadata": {
        "id": "-PXpRVEiTSaP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate descriptive statistics of the numerical columns\n",
        "pd.describe()"
      ],
      "metadata": {
        "id": "tN6GkkgXThXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing values in each column and sum them up\n",
        "pd.isna().sum()"
      ],
      "metadata": {
        "id": "vJSSNggSTkvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the DataFrame (returns a boolean DataFrame)\n",
        "pd.isnull()"
      ],
      "metadata": {
        "id": "9CtKTRoWTpRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the percentage of missing values for each column\n",
        "pd.isnull().sum()*100/len(pd)"
      ],
      "metadata": {
        "id": "ZRiYy6FoT2ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute missing values in 'Returns' with the mode\n",
        "mode_returns = pd['Returns'].mode()[0]\n",
        "pd['Returns'].fillna(mode_returns, inplace=True)\n",
        "\n",
        "# Verify that there are no more missing values in 'Returns'\n",
        "display(pd.isna().sum())"
      ],
      "metadata": {
        "id": "wwHiSMCqWF-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R3MoMsdMWohl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cce737fd"
      },
      "source": [
        "The missing values in the 'Returns' column were imputed with the mode in the previous step (`wwHiSMCqWF-3`). This is one method for handling nulls in a dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1330333f"
      },
      "source": [
        "# Task\n",
        "Analyze the data, build a model to predict churn, and evaluate the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80877381"
      },
      "source": [
        "## Exploratory data analysis (eda)\n",
        "\n",
        "### Subtask:\n",
        "Analyze the data to understand the distributions of features, identify patterns, and explore relationships between features and the target variable ('Churn'). This may include visualizations and summary statistics.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d092b680"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the requested visualizations to analyze the data, focusing on distributions and relationships with the 'Churn' variable, as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f79c083"
      },
      "source": [
        "# 1. Histogram for Customer Age distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data=pd, x='Customer Age', kde=True)\n",
        "plt.xlabel('Customer Age')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of Customer Age')\n",
        "plt.show()\n",
        "\n",
        "# 2. Count plot for Gender and Churn relationship\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=pd, x='Gender', hue='Churn')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Relationship between Gender and Churn')\n",
        "plt.show()\n",
        "\n",
        "# 3. Count plot for Product Category and Churn relationship\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.countplot(data=pd, x='Product Category', hue='Churn')\n",
        "plt.xlabel('Product Category')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Relationship between Product Category and Churn')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4. Box plot for Total Purchase Amount and Churn relationship\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.boxplot(data=pd, x='Churn', y='Total Purchase Amount')\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Total Purchase Amount')\n",
        "plt.title('Relationship between Total Purchase Amount and Churn')\n",
        "plt.show()\n",
        "\n",
        "# 5. Count plot for Payment Method and Churn relationship\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.countplot(data=pd, x='Payment Method', hue='Churn')\n",
        "plt.xlabel('Payment Method')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Relationship between Payment Method and Churn')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 6. Count plot for Returns and Churn relationship\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.countplot(data=pd, x='Returns', hue='Churn')\n",
        "plt.xlabel('Returns')\n",
        "plt.ylabel('Count')\n",
        "plt.title('Relationship between Returns and Churn')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86b728fa"
      },
      "source": [
        "## Data preprocessing\n",
        "\n",
        "### Subtask:\n",
        "Prepare the data for modeling. This may involve handling categorical variables (e.g., one-hot encoding), scaling numerical features, and splitting the data into training and testing sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf8d13a7"
      },
      "source": [
        "**Reasoning**:\n",
        "Select relevant features, apply one-hot encoding to categorical features, define X and y, and split the data into training and testing sets.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96a6bb45"
      },
      "source": [
        "**Reasoning**:\n",
        "The error indicates that the variable `pd` is being used as a DataFrame object when it was overwritten as the DataFrame itself in a previous cell. I need to use the correct variable name for the DataFrame, which should be `df` or similar, and then use `pd.get_dummies` to perform one-hot encoding.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cec95b56"
      },
      "source": [
        "**Reasoning**:\n",
        "The error persists because the variable `pd` is still being used as the DataFrame itself, not the pandas library. I need to correct this by using `pandas.get_dummies` or by assigning the DataFrame to a different variable name like `df` and then using `pd.get_dummies(df, ...)`. I will assign the dataframe to `df` and use `pd.get_dummies`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9a0e412e"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Display the shapes of the resulting sets\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DrkNvxQTYZy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b05d05a1"
      },
      "source": [
        "## Model Selection and Building\n",
        "\n",
        "### Subtask:\n",
        "Choose an appropriate model for binary classification (churn prediction) and train it on the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79092928"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd # Ensure pandas is imported as pd\n",
        "\n",
        "# Select features for modeling\n",
        "features = ['Product Category', 'Product Price', 'Quantity', 'Total Purchase Amount',\n",
        "            'Payment Method', 'Customer Age', 'Returns', 'Gender']\n",
        "X = df[features]\n",
        "y = df['Churn']\n",
        "\n",
        "# Apply one-hot encoding to categorical features on the selected features\n",
        "categorical_features = ['Product Category', 'Payment Method', 'Gender']\n",
        "X = pd.get_dummies(X, columns=categorical_features, drop_first=True)\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Display the shapes of the resulting sets and the head of the processed X\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of X_test:\", X_test.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n",
        "display(X.head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZNWoRYjQYs4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b381cf3d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize and train the Logistic Regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9e2ee41"
      },
      "source": [
        "## Model Evaluation\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the performance of the trained model using appropriate evaluation metrics (e.g., accuracy, precision, recall, F1-score, AUC)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "639691f2"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the model using various metrics\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred_proba)\n",
        "\n",
        "# Display the evaluation metrics\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n",
        "print(\"AUC:\", auc)\n",
        "\n",
        "# (Optional) Perform cross-validation for a more robust evaluation\n",
        "# cross_val_scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
        "# print(\"\\nCross-validation Accuracy Scores:\", cross_val_scores)\n",
        "# print(\"Mean Cross-validation Accuracy:\", cross_val_scores.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "530de677"
      },
      "source": [
        "# Check the distribution of the target variable 'Churn' to identify potential imbalance\n",
        "churn_distribution = df['Churn'].value_counts(normalize=True)\n",
        "print(\"Churn Distribution:\")\n",
        "print(churn_distribution)\n",
        "\n",
        "# Visualize the distribution of Churn\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=df, x='Churn')\n",
        "plt.title('Distribution of Churn')\n",
        "plt.xlabel('Churn')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7c7c430f"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize and train the Random Forest model, using class_weight='balanced' to handle imbalance\n",
        "rf_model = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1e33701"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# Predict on the test set using the Random Forest model\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluate the Random Forest model using various metrics\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "precision_rf = precision_score(y_test, y_pred_rf)\n",
        "recall_rf = recall_score(y_test, y_pred_rf)\n",
        "f1_rf = f1_score(y_test, y_pred_rf)\n",
        "auc_rf = roc_auc_score(y_test, y_pred_proba_rf)\n",
        "\n",
        "# Display the evaluation metrics for the Random Forest model\n",
        "print(\"Random Forest Model Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1-score:\", f1_rf)\n",
        "print(\"AUC:\", auc_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sHFuWOY9b_Ev"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bc9a5bf"
      },
      "source": [
        "# Print the evaluation results of the Random Forest model\n",
        "print(\"Random Forest Model Evaluation Results:\")\n",
        "print(\"Accuracy:\", accuracy_rf)\n",
        "print(\"Precision:\", precision_rf)\n",
        "print(\"Recall:\", recall_rf)\n",
        "print(\"F1-score:\", f1_rf)\n",
        "print(\"AUC:\", auc_rf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0713791"
      },
      "source": [
        "# E-commerce Customer Churn Prediction\n",
        "\n",
        "This project aims to analyze e-commerce customer data and build a model to predict customer churn.\n",
        "\n",
        "## Project Steps:\n",
        "\n",
        "1.  **Data Loading**: The dataset was loaded into a pandas DataFrame.\n",
        "2.  **Exploratory Data Analysis (EDA)**:\n",
        "    *   Initial inspection of the data using `head()`, `info()`, and `describe()`.\n",
        "    *   Checking for missing values and their percentages.\n",
        "    *   Visualizing the distribution of 'Churn' to identify class imbalance.\n",
        "3.  **Data Preprocessing**:\n",
        "    *   Handling missing values in the 'Returns' column by imputing with the mode.\n",
        "    *   Selecting relevant features for the model.\n",
        "    *   Applying one-hot encoding to categorical features ('Product Category', 'Payment Method', 'Gender').\n",
        "    *   Splitting the data into training and testing sets.\n",
        "4.  **Model Selection and Building**:\n",
        "    *   Initially, a Logistic Regression model was trained. Due to poor performance on imbalanced data, a Random Forest Classifier was chosen.\n",
        "    *   A Random Forest Classifier was initialized with `class_weight='balanced'` to address data imbalance and trained on the training data.\n",
        "5.  **Model Evaluation**:\n",
        "    *   The trained models were evaluated using metrics such as Accuracy, Precision, Recall, F1-score, and AUC.\n",
        "    *   The evaluation of the Logistic Regression model showed it was not predicting the minority class.\n",
        "    *   The Random Forest model showed some improvement in predicting the minority class, but the overall performance (especially Recall and AUC) indicated further optimization or techniques for handling imbalance could be beneficial.\n",
        "\n",
        "## Results:\n",
        "\n",
        "The analysis revealed that the dataset is imbalanced, which significantly impacts model performance. While the Random Forest model was able to make some predictions on the minority class (churn), its overall predictive power was limited, highlighting the challenges of predicting churn in this imbalanced dataset.\n",
        "\n",
        "Further work could involve exploring advanced resampling techniques, hyperparameter tuning, or trying other models better suited for imbalanced classification to improve the model's performance."
      ]
    }
  ]
}